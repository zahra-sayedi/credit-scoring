{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e76c025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pypdf\n",
    "# ! pip install streamlit\n",
    "# ! pip install ollama\n",
    "# ! pip install langchain_community\n",
    "# ! pip install langchain-text-splitters\n",
    "# ! pip install python-docx\n",
    "# ! pip install faiss-gpu\n",
    "# ! pip install \"numpy<2\"\n",
    "# ! pip install torch\n",
    "# ! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ca386df",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a1621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import html\n",
    "import json\n",
    "import time\n",
    "import faiss\n",
    "import torch\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "from ollama import Client\n",
    "from docx.table import Table as _Table\n",
    "from docx.text.paragraph import Paragraph as _Paragraph\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a89772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a app.py\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"app.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff20be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a app.py\n",
    "\n",
    "ollama = Client()\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Simple RAG with Ollama & LaBSE (FAISS)\",\n",
    "    page_icon=\"üò¨\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "CHUNK_SIZE = 1500\n",
    "CHUNK_OVERLAP = 200\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/LaBSE\"\n",
    "LLM_MODEL = \"gemma2:2b\"\n",
    "FINGLISH_MAP_FILE = \"sample_finglish_map.json\"\n",
    "TOP_K = 5\n",
    "\n",
    "FIXED_DISCLAIMER_FA = \"Disclaimer: ÿß€åŸÜ Ÿæÿßÿ≥ÿÆ ÿµÿ±ŸÅÿßŸã ÿ¨ŸÜÿ®Ÿá ÿßÿ∑ŸÑÿßÿπ‚Äåÿ±ÿ≥ÿßŸÜ€å ÿØÿßÿ±ÿØ Ÿà ÿ¨ÿß€å⁄Øÿ≤€åŸÜ ŸÖÿ¥ÿßŸàÿ±Ÿá ÿ™ÿÆÿµÿµ€å ÿ≠ŸÇŸàŸÇ€å/ŸÖÿßŸÑ€å ŸÜ€åÿ≥ÿ™.\"\n",
    "TLDR_WORD_COUNT_THRESHOLD = 300\n",
    "\n",
    "DEFAULT_PROMPT_SYSTEM = (\n",
    "    \"ÿ¥ŸÖÿß €å⁄© ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ŸÖŸÜÿØ Ÿáÿ≥ÿ™€åÿØ. ÿ¥ŸÖÿß **ŸÅŸÇÿ∑** ÿ®ÿß€åÿØ ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ŸÖÿ™ŸÜ€å ⁄©Ÿá ÿ®Ÿá ÿ¥ŸÖÿß ÿØÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØ (Context) Ÿæÿßÿ≥ÿÆ ÿØŸá€åÿØ. \"\n",
    "    \"ÿß⁄Øÿ± Ÿæÿßÿ≥ÿÆ ÿ®Ÿá Ÿá€å⁄Ü Ÿàÿ¨Ÿá ÿØÿ± ŸÖÿ™ŸÜ €åÿßŸÅÿ™ ŸÜÿ¥ÿØÿå ÿ®ÿß€åÿØ ÿ®ÿß ÿπÿ®ÿßÿ±ÿ™ ÿØŸÇ€åŸÇ **¬´Ÿæÿßÿ≥ÿÆ ÿØÿ± ŸÖÿ™ŸÜ‚ÄåŸáÿß€å ÿßÿ±ÿßÿ¶Ÿá ÿ¥ÿØŸá Ÿàÿ¨ŸàÿØ ŸÜÿØÿßÿ±ÿØ.¬ª** ÿ®Ÿá ÿ≤ÿ®ÿßŸÜ ŸÅÿßÿ±ÿ≥€å Ÿæÿßÿ≥ÿÆ ÿ±ÿß ÿ¥ÿ±Ÿàÿπ ⁄©ŸÜ€åÿØ. \"\n",
    "    \"ÿßÿ≤ Ÿáÿ±⁄ØŸàŸÜŸá ⁄ØŸÖÿßŸÜŸá‚Äåÿ≤ŸÜ€åÿå ÿØÿßŸÜÿ¥ ŸÇÿ®ŸÑ€å €åÿß Ÿæÿßÿ≥ÿÆ‚ÄåŸáÿß€å ÿπŸÖŸàŸÖ€å ÿÆŸàÿØÿØÿßÿ±€å ⁄©ŸÜ€åÿØ.\"\n",
    "    )\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "st.sidebar.info(f\"Running on device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6d49bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a app.py\n",
    "\n",
    "ollama = Client()\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Simple RAG with Ollama & LaBSE (FAISS)\",\n",
    "    page_icon=\"üò¨\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "CHUNK_SIZE = 1500\n",
    "CHUNK_OVERLAP = 200\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/LaBSE\"\n",
    "LLM_MODEL = \"gemma2:2b\"\n",
    "FINGLISH_MAP_FILE = \"sample_finglish_map.json\"\n",
    "TOP_K = 5\n",
    "\n",
    "FIXED_DISCLAIMER_FA = \"Disclaimer: ÿß€åŸÜ Ÿæÿßÿ≥ÿÆ ÿµÿ±ŸÅÿßŸã ÿ¨ŸÜÿ®Ÿá ÿßÿ∑ŸÑÿßÿπ‚Äåÿ±ÿ≥ÿßŸÜ€å ÿØÿßÿ±ÿØ Ÿà ÿ¨ÿß€å⁄Øÿ≤€åŸÜ ŸÖÿ¥ÿßŸàÿ±Ÿá ÿ™ÿÆÿµÿµ€å ÿ≠ŸÇŸàŸÇ€å/ŸÖÿßŸÑ€å ŸÜ€åÿ≥ÿ™.\"\n",
    "TLDR_WORD_COUNT_THRESHOLD = 300\n",
    "\n",
    "DEFAULT_PROMPT_SYSTEM = (\n",
    "    \"ÿ¥ŸÖÿß €å⁄© ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ŸÖŸÜÿØ Ÿáÿ≥ÿ™€åÿØ. ÿ¥ŸÖÿß **ŸÅŸÇÿ∑** ÿ®ÿß€åÿØ ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ŸÖÿ™ŸÜ€å ⁄©Ÿá ÿ®Ÿá ÿ¥ŸÖÿß ÿØÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØ (Context) Ÿæÿßÿ≥ÿÆ ÿØŸá€åÿØ. \"\n",
    "    \"ÿß⁄Øÿ± Ÿæÿßÿ≥ÿÆ ÿ®Ÿá Ÿá€å⁄Ü Ÿàÿ¨Ÿá ÿØÿ± ŸÖÿ™ŸÜ €åÿßŸÅÿ™ ŸÜÿ¥ÿØÿå ÿ®ÿß€åÿØ ÿ®ÿß ÿπÿ®ÿßÿ±ÿ™ ÿØŸÇ€åŸÇ **¬´Ÿæÿßÿ≥ÿÆ ÿØÿ± ŸÖÿ™ŸÜ‚ÄåŸáÿß€å ÿßÿ±ÿßÿ¶Ÿá ÿ¥ÿØŸá Ÿàÿ¨ŸàÿØ ŸÜÿØÿßÿ±ÿØ.¬ª** ÿ®Ÿá ÿ≤ÿ®ÿßŸÜ ŸÅÿßÿ±ÿ≥€å Ÿæÿßÿ≥ÿÆ ÿ±ÿß ÿ¥ÿ±Ÿàÿπ ⁄©ŸÜ€åÿØ. \"\n",
    "    \"ÿßÿ≤ Ÿáÿ±⁄ØŸàŸÜŸá ⁄ØŸÖÿßŸÜŸá‚Äåÿ≤ŸÜ€åÿå ÿØÿßŸÜÿ¥ ŸÇÿ®ŸÑ€å €åÿß Ÿæÿßÿ≥ÿÆ‚ÄåŸáÿß€å ÿπŸÖŸàŸÖ€å ÿÆŸàÿØÿØÿßÿ±€å ⁄©ŸÜ€åÿØ.\"\n",
    "    )\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "st.sidebar.info(f\"Running on device: {DEVICE}\")\n",
    "logger.info(f\"Application started. Running on device: {DEVICE}\")\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(str(file_path))\n",
    "    docs = loader.load()\n",
    "    text = \"\\n\".join(page.page_content for page in docs)\n",
    "    return text\n",
    "\n",
    "\n",
    "class DocParser:\n",
    "    def __init__(self, file_path, include_empty=False):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.info(f\"Initializing DocParser for: {file_path}\")\n",
    "        self.doc = Document(file_path)\n",
    "        self.include_empty = include_empty\n",
    "\n",
    "    def _table_to_markdown(self, table):\n",
    "        markdown = []\n",
    "        for i, row in enumerate(table.rows):\n",
    "            row_data = [cell.text.strip() for cell in row.cells]\n",
    "            markdown.append(\"| \" + \" | \".join(row_data) + \" |\")\n",
    "            if i == 0:\n",
    "                markdown.append(\"| \" + \" | \".join([\"---\"] * len(row_data)) + \" |\")\n",
    "        return \"\\n\".join(markdown)\n",
    "\n",
    "    def _iter_blocks(self):\n",
    "        for child in self.doc.element.body:\n",
    "            if child.tag.endswith('p'):\n",
    "                yield _Paragraph(child, self.doc)\n",
    "            elif child.tag.endswith('tbl'):\n",
    "                yield _Table(child, self.doc)\n",
    "\n",
    "    def content_extract(self):\n",
    "        self.logger.info(\"Starting content extraction...\")\n",
    "        content = []\n",
    "        try:\n",
    "            for block in self._iter_blocks():\n",
    "                if isinstance(block, _Paragraph):\n",
    "                    text = block.text.strip()\n",
    "                    if text or self.include_empty:\n",
    "                        content.append(text)\n",
    "                elif isinstance(block, _Table):\n",
    "                    content.append(self._table_to_markdown(block))\n",
    "            self.logger.info(\"Content extraction successful.\")\n",
    "            return \"\\n\\n\".join(content)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during DocParser content_extract: {e}\", exc_info=True)\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "def load_json_mapping(file_path):\n",
    "    try:\n",
    "        map_path = Path(file_path)\n",
    "        if not map_path.exists():\n",
    "            logger.warning(f\"Finglish map file not found: {file_path}. Finglish embedding will be disabled.\")\n",
    "            st.warning(f\"‚ö†Ô∏è The Finglish embedding does not exist ({file_path}). Finglish embedding will be disabled\")\n",
    "            return {}\n",
    "\n",
    "        with open(map_path, 'r', encoding='utf-8') as f:\n",
    "            mapping = json.load(f)\n",
    "            if isinstance(mapping, dict):\n",
    "                logger.info(f\"Finglish map loaded with {len(mapping)} entries from {file_path}.\")\n",
    "                st.success(f\"‚úÖ Finglish embedding loaded with length {len(mapping)} from {file_path}.\")\n",
    "                return mapping\n",
    "            else:\n",
    "                logger.error(f\"File {file_path} is not a valid dictionary.\")\n",
    "                st.error(f\"‚ùå The file {file_path} does not include a valid dictionary.\")\n",
    "                return {}\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"JSON Decode Error for {file_path}: {e}\", exc_info=True)\n",
    "        st.error(f\"‚ùå The file {file_path} faced with JSON Decode Error. Make sure of the structure!\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unknown error loading Finglish embedding: {e}\", exc_info=True)\n",
    "        st.error(f\"‚ùå Unknown error while loading the Finglish embedding: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def map_finglish_to_persian(query_text, mapping_dict):\n",
    "    processed_query = query_text\n",
    "    for finglish, persian in mapping_dict.items():\n",
    "        pattern = r'\\b' + re.escape(finglish) + r'\\b'\n",
    "        processed_query = re.sub(pattern, persian, processed_query, flags=re.IGNORECASE)\n",
    "    \n",
    "    if processed_query != query_text:\n",
    "        logger.info(\"Finglish-to-Persian mapping applied to query.\")\n",
    "    return processed_query\n",
    "\n",
    "\n",
    "def filter_persian_english(text):\n",
    "    \"\"\"\n",
    "    Removes any characters that are not in the English, Persian,\n",
    "    number, or common punctuation/whitespace Unicode ranges.\n",
    "    \"\"\"\n",
    "    pattern = r'[^\\x00-\\x7F\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF]'\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5de9c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a app.py\n",
    "\n",
    "class SimpleRAGSystem:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.info(\"Initializing SimpleRAGSystem...\")\n",
    "        with st.spinner(f\"Loading embedding model '{EMBEDDING_MODEL_NAME}'...\"):\n",
    "            try:\n",
    "                self.logger.info(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "                self.model_ST = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
    "                self.embedding_dim = self.model_ST.get_sentence_embedding_dimension()\n",
    "                self.logger.info(f\"Embedding model loaded. Dimension: {self.embedding_dim}\")\n",
    "                st.success(f\"‚úÖ Embedding model loaded. Dimension: {self.embedding_dim}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error loading embedding model: {e}\", exc_info=True)\n",
    "                st.error(f\"Error loading embedding model: {e}\")\n",
    "                st.stop()\n",
    "\n",
    "\n",
    "    def extract_text_from_file(self, uploaded_file):\n",
    "        self.logger.info(f\"Attempting to extract text from file: {uploaded_file.name}\")\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:\n",
    "                tmp_file.write(uploaded_file.getvalue())\n",
    "                tmp_file_path = tmp_file.name\n",
    "\n",
    "            text = \"\"\n",
    "            file_suffix = Path(tmp_file_path).suffix\n",
    "            self.logger.info(f\"Processing temporary file: {tmp_file_path} (Type: {file_suffix})\")\n",
    "\n",
    "            if file_suffix == \".pdf\":\n",
    "                text = load_pdf(tmp_file_path)\n",
    "            elif file_suffix == \".docx\":\n",
    "                doc_parser = DocParser(tmp_file_path)\n",
    "                text = doc_parser.content_extract()\n",
    "            else:\n",
    "                self.logger.warning(f\"Unsupported file type: {file_suffix}\")\n",
    "                st.error(f\"Unsupported file type: {file_suffix}\")\n",
    "                return None\n",
    "\n",
    "            os.remove(tmp_file_path)\n",
    "            self.logger.info(f\"Successfully extracted text from {uploaded_file.name}\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to read file: {uploaded_file.name}\", exc_info=True)\n",
    "            st.error(f\"Error reading file: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def create_chunks(self, text, source_name):\n",
    "        self.logger.info(f\"Creating chunks for: {source_name}\")\n",
    "        try:\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP\n",
    "            )\n",
    "            text_chunks = splitter.split_text(text)\n",
    "\n",
    "            chunks = []\n",
    "            for i, chunk_text in enumerate(text_chunks):\n",
    "                if chunk_text.strip():\n",
    "                    chunks.append({\n",
    "                        'id': f\"{source_name}_{i}\",\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'source': source_name,\n",
    "                        'chunk_number': i\n",
    "                    })\n",
    "            self.logger.info(f\"Created {len(chunks)} chunks for {source_name}\")\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating chunks for {source_name}: {e}\", exc_info=True)\n",
    "            st.error(f\"Error creating chunks: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        self.logger.info(f\"Generating embeddings for {len(texts)} text chunks...\")\n",
    "        try:\n",
    "            embeddings = self.model_ST.encode(\n",
    "                texts,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_tensor=False,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            self.logger.info(\"Embedding generation successful.\")\n",
    "            return np.ascontiguousarray(embeddings)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating embeddings: {e}\", exc_info=True)\n",
    "            st.error(f\"Error generating embeddings: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def create_faiss_index(self, new_embeddings, current_index):\n",
    "        self.logger.info(\"Creating/updating FAISS index...\")\n",
    "        try:\n",
    "            if current_index is None:\n",
    "                index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "                index.add(new_embeddings.astype('float32'))\n",
    "                self.logger.info(f\"New FAISS index created with {index.ntotal} vectors.\")\n",
    "                st.success(f\"New FAISS index created with {index.ntotal} vectors.\")\n",
    "                return index\n",
    "            else:\n",
    "                current_index.add(new_embeddings.astype('float32'))\n",
    "                self.logger.info(f\"Appended {len(new_embeddings)} vectors. Total: {current_index.ntotal}\")\n",
    "                st.success(f\"Appended {len(new_embeddings)} vectors. Total: {current_index.ntotal}\")\n",
    "                return current_index\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating/updating FAISS index: {e}\", exc_info=True)\n",
    "            st.error(f\"Error creating/updating FAISS index: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def search_documents(self, query, text_chunks, faiss_index, top_k=TOP_K):\n",
    "        self.logger.info(f\"Searching for query: '{query[:50]}...'\")\n",
    "        if not text_chunks or faiss_index is None:\n",
    "            self.logger.warning(\"Search attempted, but no documents processed yet.\")\n",
    "            st.warning(\"No documents have been processed yet.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            query_embedding = self.model_ST.encode(\n",
    "                query,\n",
    "                convert_to_tensor=False,\n",
    "                device=DEVICE\n",
    "            ).astype('float32').reshape(1, -1)\n",
    "\n",
    "            distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "            self.logger.info(f\"FAISS search completed. Found {len(indices[0])} potential matches.\")\n",
    "\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx == -1:\n",
    "                    continue\n",
    "                chunk = text_chunks[idx]\n",
    "                score = distances[0][i]\n",
    "                documents.append(chunk['text'])\n",
    "                metadatas.append({\n",
    "                    'source': chunk['source'],\n",
    "                    'chunk_number': chunk['chunk_number'],\n",
    "                    'score': score\n",
    "                })\n",
    "            \n",
    "            self.logger.info(f\"Returning {len(documents)} valid documents.\")\n",
    "            return {'documents': [documents], 'metadatas': [metadatas]}\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error searching documents: {e}\", exc_info=True)\n",
    "            st.error(f\"Error searching documents: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def generate_answer(self, query, context, chat_history):\n",
    "        self.logger.info(f\"Generating answer with LLM ({LLM_MODEL}) for query: '{query[:50]}...'\")\n",
    "        try:\n",
    "            history_text = \"\"\n",
    "            if chat_history:\n",
    "                for i, turn in enumerate(chat_history[-5:], start=1):\n",
    "                    history_text += f\"Ÿæÿ±ÿ≥ÿ¥ {i}: {turn['query']}\\nŸæÿßÿ≥ÿÆ {i}: {turn['answer']}\\n\\n\"\n",
    "\n",
    "            prompt = f\"\"\"{DEFAULT_PROMPT_SYSTEM}\n",
    "\n",
    "    **ÿ≥Ÿàÿßÿ®ŸÇ ⁄ØŸÅÿ™⁄ØŸà ÿ™ÿß ⁄©ŸÜŸàŸÜ:**\n",
    "    {history_text}\n",
    "\n",
    "    **ÿØÿ≥ÿ™Ÿàÿ±ÿßŸÑÿπŸÖŸÑ:**\n",
    "    - ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ŸÖÿ™ŸÜ‚ÄåŸáÿß (Context) Ÿà ÿ™ÿßÿ±€åÿÆ⁄ÜŸá ⁄ØŸÅÿ™⁄ØŸàÿå ÿ®Ÿá Ÿæÿ±ÿ≥ÿ¥ ÿ¨ÿØ€åÿØ Ÿæÿßÿ≥ÿÆ ÿ®ÿØŸá.\n",
    "    - Ÿæÿßÿ≥ÿÆ ÿ®ÿß€åÿØ ÿ®Ÿá ŸÅÿßÿ±ÿ≥€å ÿ±ŸàÿßŸÜ Ÿà ÿØŸÇ€åŸÇ ÿ®ÿßÿ¥ÿØ.\n",
    "    - ÿß⁄Øÿ± Ÿæÿßÿ≥ÿÆ ÿØÿ± ÿ™ÿßÿ±€åÿÆ⁄ÜŸá ÿ¢ŸÖÿØŸá ÿßÿ≥ÿ™ÿå ÿßÿ≤ ÿ¢ŸÜ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ ŸàŸÑ€å ÿ¢ŸÜ ÿ±ÿß ÿÆŸÑÿßÿµŸá Ÿà ÿ®ÿßÿ≤ŸÜŸà€åÿ≥€å ⁄©ŸÜ.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Ÿæÿ±ÿ≥ÿ¥ ÿ¨ÿØ€åÿØ: {query}\n",
    "\n",
    "    Ÿæÿßÿ≥ÿÆ:\"\"\"\n",
    "\n",
    "            response = ollama.generate(\n",
    "                model=LLM_MODEL,\n",
    "                prompt=prompt,\n",
    "                options={\n",
    "                    'temperature': 0.7,\n",
    "                    'max_tokens': 1000\n",
    "                }\n",
    "            )\n",
    "            self.logger.info(\"LLM generation successful.\")\n",
    "            return response['response']\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating answer with Ollama: {e}\", exc_info=True)\n",
    "            st.error(f\"Error generating answer: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def summarize_text(self, text_to_summarize):\n",
    "        self.logger.info(f\"Generating summary with LLM ({LLM_MODEL})...\")\n",
    "        try:\n",
    "            prompt = f\"\"\"ÿ¥ŸÖÿß €å⁄© ÿØÿ≥ÿ™€åÿßÿ± ÿÆŸÑÿßÿµŸá‚Äåÿ≥ÿßÿ≤ Ÿáÿ≥ÿ™€åÿØ. ŸÖÿ™ŸÜ ÿ≤€åÿ± ÿ®Ÿá ÿ≤ÿ®ÿßŸÜ ŸÅÿßÿ±ÿ≥€å ÿßÿ≥ÿ™. ÿ¢ŸÜ ÿ±ÿß ÿØÿ± €å⁄© €åÿß ÿØŸà ÿ¨ŸÖŸÑŸá ÿ±ŸàÿßŸÜ Ÿà ÿØŸÇ€åŸÇ ÿÆŸÑÿßÿµŸá ⁄©ŸÜ€åÿØ:\n",
    "\n",
    "            ŸÖÿ™ŸÜ:\n",
    "            {text_to_summarize}\n",
    "\n",
    "            ÿÆŸÑÿßÿµŸá:\"\"\"\n",
    "\n",
    "            response = ollama.generate(\n",
    "                model=LLM_MODEL,\n",
    "                prompt=prompt,\n",
    "                options={\n",
    "                    'temperature': 0.3,\n",
    "                    'max_tokens': 150\n",
    "                }\n",
    "            )\n",
    "            self.logger.info(\"Summary generation successful.\")\n",
    "            return response['response'].strip()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating summary with Ollama: {e}\", exc_info=True)\n",
    "            st.error(f\"Error generating summary: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2e3127f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a app.py\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Starting main() function. Page is loading.\")\n",
    "    st.title(\"üò¨ RAG System with Ollama & LaBSE (FAISS)\")\n",
    "    st.markdown(\"Upload PDF or DOCX documents and ask questions in Farsi or English!\")\n",
    "\n",
    "    if 'rag_system' not in st.session_state:\n",
    "        logger.info(\"Initializing 'rag_system' in session state.\")\n",
    "        st.session_state.rag_system = SimpleRAGSystem()\n",
    "\n",
    "    if 'finglish_map' not in st.session_state:\n",
    "        logger.info(\"Loading 'finglish_map' into session state.\")\n",
    "        st.session_state.finglish_map = load_json_mapping(FINGLISH_MAP_FILE)\n",
    "\n",
    "    if 'processed_files' not in st.session_state:\n",
    "        st.session_state.processed_files = set()\n",
    "\n",
    "    if 'text_chunks' not in st.session_state:\n",
    "        st.session_state.text_chunks = []\n",
    "\n",
    "    if 'faiss_index' not in st.session_state:\n",
    "        st.session_state.faiss_index = None\n",
    "\n",
    "    if 'chat_history' not in st.session_state:\n",
    "        st.session_state.chat_history = []\n",
    "\n",
    "    MAX_FILES = 5\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"üìÅ Document Upload\")\n",
    "\n",
    "        current_file_count = len(st.session_state.processed_files)\n",
    "        remaining_slots = MAX_FILES - current_file_count\n",
    "\n",
    "        st.caption(f\"File limit: {current_file_count} / {MAX_FILES} processed.\")\n",
    "\n",
    "        if remaining_slots > 0:\n",
    "            uploaded_files = st.file_uploader(\n",
    "                f\"Choose PDF or DOCX files (up to {remaining_slots} more)\",\n",
    "                type=[\"pdf\", \"docx\"],\n",
    "                help=f\"You can upload a total of {MAX_FILES} documents. You have {remaining_slots} slot(s) left.\",\n",
    "                accept_multiple_files=True\n",
    "            )\n",
    "\n",
    "            if uploaded_files:\n",
    "                if len(uploaded_files) > remaining_slots:\n",
    "                    logger.warning(f\"Upload failed: Too many files. Tried {len(uploaded_files)}, only {remaining_slots} slots left.\")\n",
    "                    st.error(\n",
    "                        f\"Upload failed. You tried to upload {len(uploaded_files)} file(s), but you only have {remaining_slots} slot(s) left. Please select fewer files.\")\n",
    "                else:\n",
    "                    needs_rerun = False\n",
    "                    start_chunk_idx = len(st.session_state.text_chunks)\n",
    "\n",
    "                    for uploaded_file in uploaded_files:\n",
    "                        file_name = uploaded_file.name\n",
    "\n",
    "                        if file_name not in st.session_state.processed_files:\n",
    "                            logger.info(f\"Processing new file: {file_name}\")\n",
    "                            with st.spinner(f\"Processing {file_name}...\"):\n",
    "\n",
    "                                text = st.session_state.rag_system.extract_text_from_file(uploaded_file)\n",
    "\n",
    "                                if text:\n",
    "                                    new_chunks = st.session_state.rag_system.create_chunks(text, file_name)\n",
    "                                    st.info(f\"Created {len(new_chunks)} chunks from {file_name}.\")\n",
    "\n",
    "                                    if new_chunks:\n",
    "                                        new_texts = [chunk['text'] for chunk in new_chunks]\n",
    "                                        new_embeddings = st.session_state.rag_system.generate_embeddings(new_texts)\n",
    "\n",
    "                                        if new_embeddings is not None:\n",
    "                                            st.session_state.text_chunks.extend(new_chunks)\n",
    "                                            st.session_state.faiss_index = st.session_state.rag_system.create_faiss_index(\n",
    "                                                new_embeddings,\n",
    "                                                st.session_state.faiss_index\n",
    "                                            )\n",
    "                                            st.session_state.processed_files.add(file_name)\n",
    "                                            logger.info(f\"Successfully processed and indexed {file_name}.\")\n",
    "                                            st.success(f\"‚úÖ Successfully processed {file_name}\")\n",
    "                                            needs_rerun = True\n",
    "                                        else:\n",
    "                                            logger.error(f\"Failed to generate embeddings for {file_name}. Skipping file.\")\n",
    "                                            st.error(f\"‚ùå Failed to generate embeddings for {file_name}\")\n",
    "                                    else:\n",
    "                                        logger.error(f\"No text chunks created for {file_name}. Skipping file.\")\n",
    "                                        st.error(f\"‚ùå No text chunks created for {file_name}\")\n",
    "                                else:\n",
    "                                    logger.error(f\"Failed to extract text from {file_name}. Skipping file.\")\n",
    "                                    st.error(f\"‚ùå Failed to extract text from {file_name}\")\n",
    "                        else:\n",
    "                            logger.warning(f\"File '{file_name}' was skipped as it has already been processed.\")\n",
    "                            st.warning(f\"File '{file_name}' has already been processed and was skipped.\")\n",
    "\n",
    "                    if needs_rerun:\n",
    "                        st.rerun()\n",
    "        else:\n",
    "            st.warning(\"You have reached the maximum of 5 processed files. No more files can be added.\")\n",
    "\n",
    "            if st.session_state.processed_files:\n",
    "                st.subheader(\"üìö Processed Documents\")\n",
    "                for file_name in st.session_state.processed_files:\n",
    "                    st.text(f\"‚Ä¢ {file_name}\")\n",
    "                total_chunks = st.session_state.faiss_index.ntotal if st.session_state.faiss_index else 0\n",
    "                st.caption(f\"Total chunks in FAISS index: {total_chunks}\")\n",
    "                logger.info(f\"Displaying processed files. Total chunks: {total_chunks}\")\n",
    "\n",
    "    if st.session_state.faiss_index is not None and st.session_state.faiss_index.ntotal > 0:\n",
    "        st.header(\"üí¨ Ask Questions\")\n",
    "        \n",
    "        \n",
    "        st.markdown(\"\"\"\n",
    "        <style>\n",
    "        input {\n",
    "        unicode-bidi: bidi-override;\n",
    "        direction: RTL;\n",
    "        text-align: right !important;\n",
    "        }\n",
    "        div[data-testid=\"stTextInput\"] span {\n",
    "        display: none !important;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "                \n",
    "        query = st.text_input(\n",
    "            \"Enter your question (preferably in Farsi):\",\n",
    "            placeholder=\"ÿß€åŸÜ ÿßÿ≥ŸÜÿßÿØ ÿØÿ± ŸÖŸàÿ±ÿØ ⁄Ü€åÿ≥ÿ™ÿü\"\n",
    "        )\n",
    "\n",
    "        if query:\n",
    "            logger.info(f\"New query received: '{query}'\")\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            processed_query = map_finglish_to_persian(query, st.session_state.finglish_map)\n",
    "            if processed_query != query:\n",
    "                logger.info(f\"Query mapped from Finglish to: '{processed_query}'\")\n",
    "                st.info(f\"You query has been modified automatically: ¬´{processed_query}¬ª\")\n",
    "\n",
    "            with st.spinner(\"üîç Searching and generating answer...\"):\n",
    "                search_results = st.session_state.rag_system.search_documents(\n",
    "                    processed_query,\n",
    "                    st.session_state.text_chunks,\n",
    "                    st.session_state.faiss_index,\n",
    "                    top_k=TOP_K\n",
    "                )\n",
    "\n",
    "                if search_results and search_results['documents']:\n",
    "                    context = \"\\n\\n---\\n\\n\".join(search_results['documents'][0])\n",
    "                    answer = st.session_state.rag_system.generate_answer(query, context, st.session_state.chat_history)\n",
    "                    answer = filter_persian_english(answer)\n",
    "                    \n",
    "                    if answer:\n",
    "                        st.subheader(\"üéØ Response\")\n",
    "                        final_answer_body = \"\"\n",
    "                        word_count = len(answer.split())\n",
    "\n",
    "                        if word_count > TLDR_WORD_COUNT_THRESHOLD:\n",
    "                            logger.info(f\"Response is long ({word_count} words). Generating summary.\")\n",
    "                            with st.spinner(f\"The response it too long ({word_count} words). Generating summary...\"):\n",
    "                                tldr_text = st.session_state.rag_system.summarize_text(answer)\n",
    "                            if tldr_text:\n",
    "                                final_answer_body = f\"**Summary:** {tldr_text}\\n\\n---\\n\\n{answer}\"\n",
    "                            else:\n",
    "                                logger.warning(\"Summary generation failed, using full answer.\")\n",
    "                                final_answer_body = answer\n",
    "                        else:\n",
    "                            final_answer_body = answer\n",
    "\n",
    "                        end_time = time.perf_counter()\n",
    "                        latency = end_time - start_time\n",
    "                        logger.info(f\"Total query latency: {latency:.2f} seconds.\")\n",
    "\n",
    "                        st.sidebar.info(f\"Latency: {latency:.2f} seconds\")\n",
    "                        \n",
    "                        escaped_answer = html.escape(final_answer_body).replace(\"\\n\", \"<br>\")\n",
    "                        escaped_disclaimer = html.escape(FIXED_DISCLAIMER_FA)\n",
    "\n",
    "                        rtl_answer_block = f\"\"\"\n",
    "                        <div dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif;\">\n",
    "                            {escaped_answer}\n",
    "                            <br><br>\n",
    "                            <hr style=\"border: 1px solid #ccc;\">\n",
    "                            <i>{escaped_disclaimer}</i>\n",
    "                        </div>\n",
    "                        \"\"\"\n",
    "                        st.markdown(rtl_answer_block, unsafe_allow_html=True)\n",
    "\n",
    "                        st.session_state.chat_history.append({\n",
    "                            \"query\": query,\n",
    "                            \"answer\": final_answer_body\n",
    "                        })\n",
    "                        if len(st.session_state.chat_history) > 5:\n",
    "                            st.session_state.chat_history.pop(0)\n",
    "\n",
    "                        with st.expander(\"üìñ Sources\", expanded=False):\n",
    "\n",
    "                            num_sources_to_show = min(3, len(search_results['documents'][0]))\n",
    "\n",
    "                            for i in range(num_sources_to_show):\n",
    "                                doc = search_results['documents'][0][i]\n",
    "                                metadata = search_results['metadatas'][0][i]\n",
    "\n",
    "                                st.markdown(\n",
    "                                    f\"<div dir='rtl' style='text-align: right;'>\"\n",
    "                                    f\"**ŸÖŸÜÿ®ÿπ {i + 1}:** {metadata['source']} (ÿ®ÿÆÿ¥ {metadata['chunk_number']})\"\n",
    "                                    f\"</div>\",\n",
    "                                    unsafe_allow_html=True\n",
    "                                )\n",
    "                                st.markdown(f\"<div dir='rtl'>**ŸÅÿßÿµŸÑŸá (L2 Distance):** {metadata['score']:.4f}</div>\",\n",
    "                                            unsafe_allow_html=True)\n",
    "\n",
    "                                rtl_doc_content = (\n",
    "                                    f\"<div dir='rtl' style='text-align: right; \"\n",
    "                                    f\"border: 1px solid #ccc; padding: 10px; border-radius: 5px; \"\n",
    "                                    f\"max-height: 150px; overflow-y: auto; white-space: pre-wrap;'>\"\n",
    "                                    f\"**ŸÖÿ≠ÿ™Ÿàÿß:** {doc}\"\n",
    "                                    f\"</div>\"\n",
    "                                )\n",
    "                                st.markdown(rtl_doc_content, unsafe_allow_html=True)\n",
    "                                st.markdown(\"---\")\n",
    "                    else:\n",
    "                        logger.error(\"LLM generation returned an empty answer.\")\n",
    "                        st.error(\"The model failed to generate a response.\")\n",
    "                else:\n",
    "                    logger.warning(f\"No relevant documents found for query: '{query}'\")\n",
    "                    st.warning(\"No relevant documents found for your query.\")\n",
    "    else:\n",
    "        st.info(\"üëÜ Please upload a PDF or DOCX document using the sidebar to get started!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b99d095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a app.py\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df735b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.17.0.10:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://31.14.121.178:8501\u001b[0m\n",
      "\u001b[0m\n",
      "2025-10-28 00:16:22,173 - __main__ - INFO - Application started. Running on device: cuda\n",
      "2025-10-28 00:16:22,184 - __main__ - INFO - Starting main() function. Page is loading.\n",
      "2025-10-28 00:16:22,187 - __main__ - INFO - Initializing 'rag_system' in session state.\n",
      "2025-10-28 00:16:22,187 - SimpleRAGSystem - INFO - Initializing SimpleRAGSystem...\n",
      "2025-10-28 00:16:22,203 - SimpleRAGSystem - INFO - Loading embedding model: sentence-transformers/LaBSE\n",
      "2025-10-28 00:16:22,207 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/LaBSE\n",
      "2025-10-28 00:16:53,929 - SimpleRAGSystem - INFO - Embedding model loaded. Dimension: 768\n",
      "2025-10-28 00:16:53,936 - __main__ - INFO - Loading 'finglish_map' into session state.\n",
      "2025-10-28 00:16:53,937 - __main__ - INFO - Finglish map loaded with 100 entries from sample_finglish_map.json.\n",
      "2025-10-28 00:17:20,654 - __main__ - INFO - Application started. Running on device: cuda\n",
      "2025-10-28 00:17:20,664 - __main__ - INFO - Starting main() function. Page is loading.\n",
      "2025-10-28 00:17:20,680 - __main__ - INFO - Processing new file: A3 credit.docx\n",
      "2025-10-28 00:17:20,699 - SimpleRAGSystem - INFO - Attempting to extract text from file: A3 credit.docx\n",
      "2025-10-28 00:17:20,702 - SimpleRAGSystem - INFO - Processing temporary file: /tmp/tmpo4w_2o01.docx (Type: .docx)\n",
      "2025-10-28 00:17:20,702 - DocParser - INFO - Initializing DocParser for: /tmp/tmpo4w_2o01.docx\n",
      "2025-10-28 00:17:20,723 - DocParser - INFO - Starting content extraction...\n",
      "2025-10-28 00:17:20,772 - DocParser - INFO - Content extraction successful.\n",
      "2025-10-28 00:17:20,774 - SimpleRAGSystem - INFO - Successfully extracted text from A3 credit.docx\n",
      "2025-10-28 00:17:20,775 - SimpleRAGSystem - INFO - Creating chunks for: A3 credit.docx\n",
      "2025-10-28 00:17:20,777 - SimpleRAGSystem - INFO - Created 3 chunks for A3 credit.docx\n",
      "2025-10-28 00:17:20,784 - SimpleRAGSystem - INFO - Generating embeddings for 3 text chunks...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.70s/it]\n",
      "2025-10-28 00:17:24,518 - SimpleRAGSystem - INFO - Embedding generation successful.\n",
      "2025-10-28 00:17:24,519 - SimpleRAGSystem - INFO - Creating/updating FAISS index...\n",
      "2025-10-28 00:17:24,520 - SimpleRAGSystem - INFO - New FAISS index created with 3 vectors.\n",
      "2025-10-28 00:17:24,524 - __main__ - INFO - Successfully processed and indexed A3 credit.docx.\n",
      "2025-10-28 00:17:24,533 - __main__ - INFO - Processing new file: Arrears clearance time.docx\n",
      "2025-10-28 00:17:24,559 - SimpleRAGSystem - INFO - Attempting to extract text from file: Arrears clearance time.docx\n",
      "2025-10-28 00:17:24,563 - SimpleRAGSystem - INFO - Processing temporary file: /tmp/tmpp54j6bp7.docx (Type: .docx)\n",
      "2025-10-28 00:17:24,572 - DocParser - INFO - Initializing DocParser for: /tmp/tmpp54j6bp7.docx\n",
      "2025-10-28 00:17:24,593 - DocParser - INFO - Starting content extraction...\n",
      "2025-10-28 00:17:24,602 - DocParser - INFO - Content extraction successful.\n",
      "2025-10-28 00:17:24,603 - SimpleRAGSystem - INFO - Successfully extracted text from Arrears clearance time.docx\n",
      "2025-10-28 00:17:24,604 - SimpleRAGSystem - INFO - Creating chunks for: Arrears clearance time.docx\n",
      "2025-10-28 00:17:24,604 - SimpleRAGSystem - INFO - Created 3 chunks for Arrears clearance time.docx\n",
      "2025-10-28 00:17:24,607 - SimpleRAGSystem - INFO - Generating embeddings for 3 text chunks...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.41it/s]\n",
      "2025-10-28 00:17:25,045 - SimpleRAGSystem - INFO - Embedding generation successful.\n",
      "2025-10-28 00:17:25,046 - SimpleRAGSystem - INFO - Creating/updating FAISS index...\n",
      "2025-10-28 00:17:25,046 - SimpleRAGSystem - INFO - Appended 3 vectors. Total: 6\n",
      "2025-10-28 00:17:25,048 - __main__ - INFO - Successfully processed and indexed Arrears clearance time.docx.\n",
      "2025-10-28 00:17:25,077 - __main__ - INFO - Processing new file: bank-melli-300m-loan.docx\n",
      "2025-10-28 00:17:25,091 - SimpleRAGSystem - INFO - Attempting to extract text from file: bank-melli-300m-loan.docx\n",
      "2025-10-28 00:17:25,094 - SimpleRAGSystem - INFO - Processing temporary file: /tmp/tmpacq0fh2x.docx (Type: .docx)\n",
      "2025-10-28 00:17:25,102 - DocParser - INFO - Initializing DocParser for: /tmp/tmpacq0fh2x.docx\n",
      "2025-10-28 00:17:25,134 - DocParser - INFO - Starting content extraction...\n",
      "2025-10-28 00:17:25,159 - DocParser - INFO - Content extraction successful.\n",
      "2025-10-28 00:17:25,160 - SimpleRAGSystem - INFO - Successfully extracted text from bank-melli-300m-loan.docx\n",
      "2025-10-28 00:17:25,160 - SimpleRAGSystem - INFO - Creating chunks for: bank-melli-300m-loan.docx\n",
      "2025-10-28 00:17:25,161 - SimpleRAGSystem - INFO - Created 6 chunks for bank-melli-300m-loan.docx\n",
      "2025-10-28 00:17:25,163 - SimpleRAGSystem - INFO - Generating embeddings for 6 text chunks...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.97it/s]\n",
      "2025-10-28 00:17:25,723 - SimpleRAGSystem - INFO - Embedding generation successful.\n",
      "2025-10-28 00:17:25,724 - SimpleRAGSystem - INFO - Creating/updating FAISS index...\n",
      "2025-10-28 00:17:25,725 - SimpleRAGSystem - INFO - Appended 6 vectors. Total: 12\n",
      "2025-10-28 00:17:25,728 - __main__ - INFO - Successfully processed and indexed bank-melli-300m-loan.docx.\n",
      "2025-10-28 00:17:25,730 - __main__ - INFO - Processing new file: credit-scores.docx\n",
      "2025-10-28 00:17:25,747 - SimpleRAGSystem - INFO - Attempting to extract text from file: credit-scores.docx\n",
      "2025-10-28 00:17:25,751 - SimpleRAGSystem - INFO - Processing temporary file: /tmp/tmpt9xrbiun.docx (Type: .docx)\n",
      "2025-10-28 00:17:25,760 - DocParser - INFO - Initializing DocParser for: /tmp/tmpt9xrbiun.docx\n",
      "2025-10-28 00:17:25,812 - DocParser - INFO - Starting content extraction...\n",
      "2025-10-28 00:17:25,855 - DocParser - INFO - Content extraction successful.\n",
      "2025-10-28 00:17:25,875 - SimpleRAGSystem - INFO - Successfully extracted text from credit-scores.docx\n",
      "2025-10-28 00:17:25,876 - SimpleRAGSystem - INFO - Creating chunks for: credit-scores.docx\n",
      "2025-10-28 00:17:25,880 - SimpleRAGSystem - INFO - Created 8 chunks for credit-scores.docx\n",
      "2025-10-28 00:17:25,885 - SimpleRAGSystem - INFO - Generating embeddings for 8 text chunks...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s]\n",
      "2025-10-28 00:17:26,213 - SimpleRAGSystem - INFO - Embedding generation successful.\n",
      "2025-10-28 00:17:26,226 - SimpleRAGSystem - INFO - Creating/updating FAISS index...\n",
      "2025-10-28 00:17:26,228 - SimpleRAGSystem - INFO - Appended 8 vectors. Total: 20\n",
      "2025-10-28 00:17:26,232 - __main__ - INFO - Successfully processed and indexed credit-scores.docx.\n",
      "2025-10-28 00:17:26,286 - __main__ - INFO - Processing new file: increase-credit.docx\n",
      "2025-10-28 00:17:26,316 - SimpleRAGSystem - INFO - Attempting to extract text from file: increase-credit.docx\n",
      "2025-10-28 00:17:26,319 - SimpleRAGSystem - INFO - Processing temporary file: /tmp/tmpb1g5vraf.docx (Type: .docx)\n",
      "2025-10-28 00:17:26,347 - DocParser - INFO - Initializing DocParser for: /tmp/tmpb1g5vraf.docx\n",
      "2025-10-28 00:17:26,411 - DocParser - INFO - Starting content extraction...\n",
      "2025-10-28 00:17:26,430 - DocParser - INFO - Content extraction successful.\n",
      "2025-10-28 00:17:26,433 - SimpleRAGSystem - INFO - Successfully extracted text from increase-credit.docx\n",
      "2025-10-28 00:17:26,434 - SimpleRAGSystem - INFO - Creating chunks for: increase-credit.docx\n",
      "2025-10-28 00:17:26,435 - SimpleRAGSystem - INFO - Created 5 chunks for increase-credit.docx\n",
      "2025-10-28 00:17:26,439 - SimpleRAGSystem - INFO - Generating embeddings for 5 text chunks...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.98it/s]\n",
      "2025-10-28 00:17:26,584 - SimpleRAGSystem - INFO - Embedding generation successful.\n",
      "2025-10-28 00:17:26,586 - SimpleRAGSystem - INFO - Creating/updating FAISS index...\n",
      "2025-10-28 00:17:26,588 - SimpleRAGSystem - INFO - Appended 5 vectors. Total: 25\n",
      "2025-10-28 00:17:26,604 - __main__ - INFO - Successfully processed and indexed increase-credit.docx.\n",
      "2025-10-28 00:17:29,090 - __main__ - INFO - Application started. Running on device: cuda\n",
      "2025-10-28 00:17:29,124 - __main__ - INFO - Starting main() function. Page is loading.\n",
      "2025-10-28 00:17:29,177 - __main__ - INFO - Displaying processed files. Total chunks: 25\n",
      "2025-10-28 00:17:45,771 - __main__ - INFO - Application started. Running on device: cuda\n",
      "2025-10-28 00:17:45,772 - __main__ - INFO - Starting main() function. Page is loading.\n",
      "2025-10-28 00:17:45,778 - __main__ - INFO - Displaying processed files. Total chunks: 25\n",
      "2025-10-28 00:17:45,781 - __main__ - INFO - New query received: 'ÿßŸÖÿ™€åÿßÿ≤ a3 ⁄Ü€åÿ≥ÿ™ ÿü'\n",
      "2025-10-28 00:17:45,848 - SimpleRAGSystem - INFO - Searching for query: 'ÿßŸÖÿ™€åÿßÿ≤ a3 ⁄Ü€åÿ≥ÿ™ ÿü...'\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.28it/s]\n",
      "2025-10-28 00:17:45,963 - SimpleRAGSystem - INFO - FAISS search completed. Found 5 potential matches.\n",
      "2025-10-28 00:17:45,964 - SimpleRAGSystem - INFO - Returning 5 valid documents.\n",
      "2025-10-28 00:17:45,965 - SimpleRAGSystem - INFO - Generating answer with LLM (gemma2:2b) for query: 'ÿßŸÖÿ™€åÿßÿ≤ a3 ⁄Ü€åÿ≥ÿ™ ÿü...'\n",
      "2025-10-28 00:17:46,566 - httpx - INFO - HTTP Request: POST http://172.17.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-10-28 00:17:46,569 - SimpleRAGSystem - INFO - LLM generation successful.\n",
      "2025-10-28 00:17:46,579 - __main__ - INFO - Total query latency: 0.79 seconds.\n",
      "2025-10-28 00:19:16,535 - __main__ - INFO - Application started. Running on device: cuda\n",
      "2025-10-28 00:19:16,536 - __main__ - INFO - Starting main() function. Page is loading.\n",
      "2025-10-28 00:19:16,571 - __main__ - INFO - Displaying processed files. Total chunks: 25\n",
      "2025-10-28 00:19:16,574 - __main__ - INFO - New query received: 'ŸáŸàÿß ⁄Üÿ∑Ÿàÿ±Ÿáÿü'\n",
      "2025-10-28 00:19:16,591 - SimpleRAGSystem - INFO - Searching for query: 'ŸáŸàÿß ⁄Üÿ∑Ÿàÿ±Ÿáÿü...'\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.42it/s]\n",
      "2025-10-28 00:19:16,699 - SimpleRAGSystem - INFO - FAISS search completed. Found 5 potential matches.\n",
      "2025-10-28 00:19:16,700 - SimpleRAGSystem - INFO - Returning 5 valid documents.\n",
      "2025-10-28 00:19:16,700 - SimpleRAGSystem - INFO - Generating answer with LLM (gemma2:2b) for query: 'ŸáŸàÿß ⁄Üÿ∑Ÿàÿ±Ÿáÿü...'\n",
      "2025-10-28 00:19:17,636 - httpx - INFO - HTTP Request: POST http://172.17.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-10-28 00:19:17,646 - SimpleRAGSystem - INFO - LLM generation successful.\n",
      "2025-10-28 00:19:17,654 - __main__ - INFO - Total query latency: 1.08 seconds.\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/weakref.py\", line 667, in _exitfunc\n",
      "    f()\n",
      "  File \"/usr/lib/python3.10/weakref.py\", line 591, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"/root/RAG/.rag/.venv/lib/python3.10/site-packages/torch/library.py\", line 462, in _del_library\n",
      "    m.reset()\n",
      "  File \"/root/RAG/.rag/.venv/lib/python3.10/site-packages/streamlit/web/bootstrap.py\", line 42, in signal_handler\n",
      "    server.stop()\n",
      "  File \"/root/RAG/.rag/.venv/lib/python3.10/site-packages/streamlit/web/server/server.py\", line 510, in stop\n",
      "    self._runtime.stop()\n",
      "  File \"/root/RAG/.rag/.venv/lib/python3.10/site-packages/streamlit/runtime/runtime.py\", line 329, in stop\n",
      "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 798, in call_soon_threadsafe\n",
      "    self._check_closed()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n"
     ]
    }
   ],
   "source": [
    "! streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2b876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278b7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
